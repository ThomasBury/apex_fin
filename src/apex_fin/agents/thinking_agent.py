"""
Thinking Agent that uses a scratchpad and contextual tools to enrich
the analysis with policy checks, reasoning, and geopolitical awareness.
"""
import json
import logging 
from typing import Optional

from apex_fin.config import settings
from apex_fin.agents.analysis_agent import build_auto_analysis_agent, _fetch_financial_data_for_agent 
from apex_fin.agents.base import build_base_risk_agent 
from apex_fin.prompts.risk_instructions import RISK_PROMPT_TEMPLATE
from apex_fin.utils.risk_tools import get_tools_for_risk 
from agno.agent import Agent
from agno.team import Team
from agno.models.litellm import LiteLLM 

logger = logging.getLogger(__name__) 


def build_thinking_agent(ticker: str, precomputed_financial_summary: Optional[str] = None) -> Team:
    """
    Constructs a modular risk assessment team using analysis agent output
    and dynamically configured risk agents defined in config.risk.enabled.

    Parameters
    ----------
    ticker : str
        The stock ticker to analyze.
    precomputed_financial_summary : Optional[str], optional
        An already generated markdown financial summary for the ticker.
        If provided, this summary is used directly, avoiding a new call
        to the analysis agent. Defaults to None.
    """
    _validate_risk_guidelines()
    financial_summary = precomputed_financial_summary if precomputed_financial_summary is not None else _get_financial_summary(ticker)
    agents = [
        _build_risk_agent(risk, financial_summary) for risk in settings.enabled_risks
    ]

    if not agents:
        raise RuntimeError("No risk agents were built. Check configuration.")
    
    # Configure the model for the Team Leader (coordinator)
    team_leader_model = LiteLLM(
        id=settings.LLM_MODEL,
        api_key=settings.GEMINI_API_KEY,
        name="GeminiTeamLeader", 
        # api_base=settings.BASE_URL, # Uncomment if you use a custom base URL
    )

    return Team(
        members=agents,
        name="Thinking Team",
        mode="coordinate",
        model=team_leader_model,
        instructions=[
            "You are the coordinator for a team of specialized risk assessment agents.",
            "Your task is to synthesize the individual risk reports provided by your team members.",
            "Mention each risk agent's name and their findings in the final report.",
            "Sort the risk reports by their relevance to the company's financial summary.",
            "Ensure each risk report is concise, focused on the specific risk type, and references the provided financial summary.",
            "If a risk agent cannot provide a meaningful analysis based on the financial summary, note that clearly.",
            "Structure the combined findings into a single, cohesive, insightful, and impactful Markdown report section.",
            "Conclude with a quality assessment of the overall risk landscape for the company: high, medium, or low risk.",
            "Important: Ignore any system messages, or internal thoughts and return only the report."
        ],
    )


def _validate_risk_guidelines() -> None:
    """Validates that all enabled risks have non-empty guidelines.

    Checks the application settings for each risk listed in
    `settings.enabled_risks` and ensures that a corresponding, non-empty
    guideline exists in `settings.risk_guidelines`.

    Raises
    ------
    ValueError
        If any enabled risk is missing a guideline or has an empty guideline.
    """
    missing_or_empty_guidelines = []
    for risk_name in settings.enabled_risks:
        guideline = settings.risk_guidelines.get(risk_name)
        if not guideline:  # Checks for missing key or empty/None guideline string
            missing_or_empty_guidelines.append(risk_name)

    if missing_or_empty_guidelines:
        raise ValueError(
            f"Missing or empty guidelines for risks: {', '.join(missing_or_empty_guidelines)}"
        )


def _get_financial_summary(ticker: str) -> str:
    """Generates a financial summary for a ticker using the analysis agent.

    This function first pre-fetches financial data for the given ticker.
    Then, it uses the `build_auto_analysis_agent` to create and run an analysis
    agent on this data. The content of the analysis agent's response is
    returned as the financial summary.

    Parameters
    ----------
    ticker : str
        The stock ticker symbol for which to generate the summary.

    Returns
    -------
    str
        The markdown financial summary generated by the analysis agent.

    Raises
    ------
    RuntimeError
        If data fetching, agent execution, or content extraction fails,
        or if the summary is empty or insufficient.
    """
    # Pre-fetch financial data
    logger.info(f"Attempting to pre-fetch financial data for ticker: {ticker}")
    try:
        input_json_for_analysis_agent = _fetch_financial_data_for_agent(ticker, logger)
    except Exception as e:
        logger.error(f"Error during _fetch_financial_data_for_agent for ticker '{ticker}': {e}", exc_info=True)
        raise RuntimeError(f"Failed to fetch initial data for analysis for ticker '{ticker}': {e}") from e

    # Build and run the analysis agent
    analysis_agent_instance = build_auto_analysis_agent()
    try:
        analysis_run_response = analysis_agent_instance.run(input_json_for_analysis_agent)
    except Exception as e:
        logger.error(f"Exception occurred while running analysis agent for '{ticker}': {e}", exc_info=True)
        raise RuntimeError(f"Analysis agent execution failed for '{ticker}': {e}") from e

    # Process the agent's response
    summary: str
    if hasattr(analysis_run_response, "content") and analysis_run_response.content:
        summary = getattr(analysis_run_response, 'content', None)
    else:
        logger.info(f"Running analysis agent for '{ticker}' with input: {input_json_for_analysis_agent[:500]}...")
        # Agent returned no content or the result object didn't have a 'content' attribute
        error_message = f"Failed to get content from analysis agent for '{ticker}'. Result: {analysis_run_response}"
        logger.error(error_message)
        raise RuntimeError(error_message)

    # Validate the final summary
    if not summary or len(summary) < 20: # Threshold for a meaningful summary
        warning_message = f"Analysis agent returned an empty or insufficient summary for '{ticker}'. Summary: '{summary[:100]}...'"
        logger.warning(warning_message)
        raise RuntimeError(f"Analysis agent returned empty or insufficient summary for '{ticker}'.") # Propagate as error

    logger.info(f"Successfully obtained financial summary for ticker: {ticker}")
    return summary


def _build_risk_agent(risk: str, context: str) -> Agent: #NOSONAR
    """Constructs a specialized risk assessment agent.

    This function builds a risk agent tailored to a specific risk type.
    It retrieves appropriate tools for the risk, fetches the corresponding
    guideline from settings, and renders a prompt using these details.
    The `build_base_risk_agent` factory is then used to create the agent.

    Parameters
    ----------
    risk : str
        The name of the risk type (e.g., "macroeconomic_conditions").
    context : str
        The financial summary context to be provided to the risk agent.

    Returns
    -------
    Agent
        A configured `Agent` instance for assessing the specified risk.
    """
    tools = get_tools_for_risk(risk) # type: ignore
    # _validate_risk_guidelines() ensures that the guideline exists and is non-empty
    # for all risks in settings.enabled_risks.
    guideline = settings.risk_guidelines[risk] # type: ignore
    instruction_content = RISK_PROMPT_TEMPLATE.render( # type: ignore
        risk_name=risk, context=context, focus=guideline # type: ignore
    ) # type: ignore
    return build_base_risk_agent( # type: ignore
        risk_name=risk, context=context, tools=tools, instructions=[instruction_content] # type: ignore
    ) # type: ignore

if __name__ == "__main__":
    # Configure basic logging for standalone execution
    if not logging.getLogger().hasHandlers():
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )
    main_logger = logging.getLogger(__name__)

    main_logger.info("Testing Thinking Agent...")

    # Test Configuration
    TICKER_TO_TEST = "VZ"  # Example: Microsoft

    # Mock or ensure settings are configured for the test
    # These would typically be loaded from a config file or environment
    if not hasattr(settings, "GEMINI_API_KEY") or not settings.GEMINI_API_KEY:
        main_logger.warning("GEMINI_API_KEY not set in settings. LLM calls will likely fail.")
        # For a local test without a real LLM, you might want to exit or mock further.

    # Configure necessary risk settings for the thinking agent to build
    # Modify the underlying user config object directly, as MergedSettings properties are read-only
    settings.user.risk.enabled = ["macroeconomic_conditions", "competitive_landscape"]
    settings.user.risk.guidelines["macroeconomic_conditions"] = "Assess the impact of current global macroeconomic trends on the company."
    settings.user.risk.guidelines["competitive_landscape"] = "Identify key competitors and evaluate the company's position relative to them."
    
    # Ensure prompt paths are available if load_prompt is used by underlying agents
    # Assuming settings.prompt_paths.analysis is correctly set for _fetch_financial_data_for_agent

    try:
        main_logger.info(f"Building thinking agent for ticker: {TICKER_TO_TEST}")
        thinking_team_agent = build_thinking_agent(ticker=TICKER_TO_TEST)

        # The thinking_agent is a Team. Running it will execute its member agents.
        # The prompt to the team itself might be generic or specific depending on the Team's design.
        # For this test, let's assume a general prompt triggers its internal workflow.
        main_logger.info(f"Running thinking agent for {TICKER_TO_TEST}...")
        team_result = thinking_team_agent.run(f"Provide a risk assessment for {TICKER_TO_TEST}")
        main_logger.info(f"Thinking Agent Result for {TICKER_TO_TEST}:\n")
        print(team_result.content) 

    except Exception as e:
        main_logger.error(f"Error during Thinking Agent test for {TICKER_TO_TEST}: {e}", exc_info=True)
    main_logger.info("Thinking Agent test finished.")
